{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsDRVWdCCQLf"
      },
      "source": [
        "# Chapter 2.3 — Multi-Head Attention: Parallel “Views” of Meaning\n",
        "\n",
        "*Understanding how multiple attention heads learn different aspects of context.*\n",
        "\n",
        "Companion article: [Medium — Chapter 2.3](https://medium.com/@vadidsadikshaikh/chapter-2-3-multi-head-attention-parallel-views-of-meaning-5c47b51b9e73)\n",
        "\n",
        "Reference: Sebastian Raschka, *Build a Large Language Model (From Scratch)*\n",
        "\n",
        "Purpose: Implement multi-head attention by splitting queries, keys, and values into multiple heads, computing attention in parallel, and concatenating the results."
      ],
      "id": "AsDRVWdCCQLf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8T1LA3wCQLh"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Reuse scaled dot-product attention from Chapter 2.1\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    d_k = Q.size(-1)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn, V)\n",
        "    return output, attn"
      ],
      "id": "K8T1LA3wCQLh",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUOVjrTyCQLi"
      },
      "source": [
        "# Define Multi-Head Attention class\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        # Linear projection and split into heads\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Compute attention for each head\n",
        "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Concatenate heads\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear layer\n",
        "        output = self.W_o(attn_output)\n",
        "        return output, attn_weights"
      ],
      "id": "QUOVjrTyCQLi",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBYWZDY4CQLi",
        "outputId": "e9640892-b3fe-42ce-aa5d-71db1f84cca3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example usage\n",
        "torch.manual_seed(0)\n",
        "batch_size, seq_len, d_model, num_heads = 1, 5, 64, 8\n",
        "\n",
        "x = torch.rand(batch_size, seq_len, d_model)\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "output, attn = mha(x, x, x)\n",
        "\n",
        "print('Output shape:', output.shape)\n",
        "print('Attention shape:', attn.shape)"
      ],
      "id": "PBYWZDY4CQLi",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 5, 64])\n",
            "Attention shape: torch.Size([1, 8, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2qEdQZ1CQLi"
      },
      "source": [
        "### Notes:\n",
        "- Each head processes a different “projection” of the same input.\n",
        "- Attention heads learn different relationships — syntax, semantics, or position.\n",
        "- Concatenating outputs allows the model to integrate diverse insights.\n",
        "- In GPT-style architectures, multi-head attention is used in every transformer block.\n",
        "\n",
        "**Next:** Chapter 2.4 — Dropout and Normalization: Keeping Learning Stable"
      ],
      "id": "e2qEdQZ1CQLi"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
