{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1.2 — Embeddings: Giving Tokens Meaning\n",
    "\n",
    "*Establishing relationships between words through meaning*\n",
    "\n",
    "Companion article: https://medium.com/@vadidsadikshaikh/chapter-1-2-embeddings-giving-tokens-meaning\n",
    "Reference: Sebastian Raschka, *Build a Large Language Model (From Scratch)*\n",
    "\n",
    "Purpose: implement a simple embedding lookup using PyTorch and visualize how token IDs map to vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why embeddings?\n",
    "\n",
    "Tokenization turns text into integers. Embeddings turn those integers into **dense vectors** that encode semantic relationships. In this notebook we:\n",
    "\n",
    "1. Encode a short sentence with the same `tiktoken` tokenizer used in Chapter 1.1.\n",
    "2. Create a PyTorch `nn.Embedding` layer and map token IDs to vectors.\n",
    "3. Inspect shapes and a truncated vector sample.\n",
    "4. (Optional) Visualize a small set of token vectors in 2D using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCA for visualization (optional). If sklearn is not available, the cell will try a simple SVD fallback.\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "    _HAS_SKLEARN = True\n",
    "except Exception:\n",
    "    _HAS_SKLEARN = False\n",
    "    from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer (GPT-2 BPE encoding) — same as Chapter 1.1\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"Loaded tokenizer:\", \"gpt2\")\n",
    "print(\"Vocab size:\", getattr(tokenizer, 'n_vocab', None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text and tokenization\n",
    "text = \"Large language models read the world as numbers.\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Text:\\n\", text)\n",
    "print(\"\\nToken IDs:\\n\", token_ids)\n",
    "print(\"Number of tokens:\", len(token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer\n",
    "vocab_size = getattr(tokenizer, 'n_vocab', 50257)\n",
    "embedding_dim = 128   # smaller dim for demo; later you can set 768 or 1024\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert token ids to a tensor (batch size 1)\n",
    "ids_tensor = torch.tensor([token_ids], dtype=torch.long)\n",
    "\n",
    "# Forward pass through embedding\n",
    "embeddings = embedding_layer(ids_tensor)  # shape: (1, seq_len, embedding_dim)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"First token vector (first 10 dims):\", embeddings[0, 0, :10].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize (PCA) — only do this for a small set of tokens to keep plot readable\n",
    "def compute_2d_projection(vectors_np):\n",
    "    if _HAS_SKLEARN:\n",
    "        pca = PCA(n_components=2)\n",
    "        return pca.fit_transform(vectors_np)\n",
    "    else:\n",
    "        # simple SVD fallback\n",
    "        X = vectors_np - vectors_np.mean(axis=0)\n",
    "        U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "        return U[:, :2]\n",
    "\n",
    "# choose tokens to visualize: unique tokens from the sentence\n",
    "unique_ids = list(dict.fromkeys(token_ids))\n",
    "vecs = embeddings[0].detach().numpy()  # shape: (seq_len, embedding_dim)\n",
    "to_plot = np.array([vecs[i] for i in range(len(unique_ids))])\n",
    "proj = compute_2d_projection(to_plot)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(proj[:,0], proj[:,1])\n",
    "for i, tid in enumerate(unique_ids):\n",
    "    token_str = tokenizer.decode([tid]).strip()\n",
    "    plt.annotate(token_str, (proj[i,0], proj[i,1]), textcoords='offset points', xytext=(4,3), ha='left', fontsize=9)\n",
    "plt.title('PCA projection of token embeddings (sample)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Next Steps\n",
    "\n",
    "- This notebook demonstrates a minimal PyTorch `nn.Embedding` layer and how token IDs map to vectors.\n",
    "- `embedding_dim` is set to 128 for demo speed — for real models you will typically see 512, 768, or 1024+.\n",
    "- These embeddings are **learnable** (randomly initialized here). During training the embedding weights update to capture semantics.\n",
    "- Next: in Chapter 1.3 we will add **positional encoding** so the model knows token order, then move on to attention mechanisms.\n",
    "\n",
    "### Companion Notebook (code)\n",
    "LLM-FROM-BASICS/1_language_for_machines/1.2_embeddings.ipynb\n",
    "GitHub implementation: https://github.com/vadidshaikh/LLM-FROM-BASICS/blob/main/1_language_for_machines/1.2_embeddings.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
